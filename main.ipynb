{"cells":[{"cell_type":"code","source":"!pip install transformers==4.26.0","metadata":{"tags":[],"cell_id":"68543d724d45459eb322e01f7cc0a0d9","source_hash":"717f0504","output_cleared":true,"execution_start":1674738333969,"execution_millis":6588,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, DistilBertForSequenceClassification\nfrom sklearn.metrics import accuracy_score\n\n# imported packages correctly?\nprint(\"successfully!\")\n\n# Load the data from the CSV file\ndata = pd.read_csv('/datasets/summaries/data.csv')\nprint(\"Read file\")\n\n# sample only 20% of the data\ndata = data.sample(frac=0.1)\nprint(\"Got sample\")\n\n# drop missing values\ndata.dropna(inplace=True)\nprint(\"Dropped empty values\")\n\n# Split the data into training and testing sets\ntrain_data, test_data = train_test_split(data, test_size=0.2)\nprint(\"split the data sets\")\n\n# Extract the summaries and full stories from the training data\ntrain_summaries = train_data['Summary']\ntrain_stories = train_data['Content']\nprint(\"extracted specific columns from training data\")\n\n# Extract the summaries and full stories from the testing data\ntest_summaries = test_data['Summary']\ntest_stories = test_data['Content']\nprint(\"extracted specific columns from testing data\")\n\n# Load the pre-trained tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\nprint(\"created the tokenizer\")\nmodel = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\nprint(\"defined the model\")\n\n# Convert the data into input format for the model\ntrain_inputs = tokenizer.batch_encode_plus(train_stories.tolist(), max_length=512, pad_to_max_length=True, return_tensors='pt')\ntest_inputs = tokenizer.batch_encode_plus(test_stories.tolist(), max_length=512, pad_to_max_length=True, return_tensors='pt')\nprint(\"tokenizied the training and the testing data\")\n\n# Train the model on the training data\nmodel.train()\nprint(\"trained the model\")\nmodel.zero_grad()\nprint(\"set all the gradients of the model's parameters to zero\")\noutputs = model(input_ids=train_inputs['input_ids'], labels=train_summaries)\nprint(\"defined the outputs\")\nloss, logits = outputs[:2]\nloss.backward()\noptimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\noptimizer.step()\n\n# Test the model on the testing data\nmodel.eval()\noutputs = model(input_ids=test_inputs['input_ids'])\npredictions = outputs[0]\n\n# Calculate the accuracy of the model\naccuracy = accuracy_score(test_summaries, predictions)\nprint('Accuracy:', accuracy)\n","metadata":{"tags":[],"cell_id":"098004dedd36496c818b961c58347b6c","source_hash":"7717b8b2","execution_start":1674738340561,"execution_millis":1687,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"/shared-libs/python3.9/py/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n2023-01-26 13:05:41.369756: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-01-26 13:05:41.488285: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2023-01-26 13:05:41.493064: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2023-01-26 13:05:41.493086: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2023-01-26 13:05:41.516423: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2023-01-26 13:05:42.540153: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n2023-01-26 13:05:42.540216: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n2023-01-26 13:05:42.540222: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\nsuccessfully!\nRead file\nGot sample\nDropped empty values\nsplit the data sets\nextracted specific columns from training data\nextracted specific columns from testing data\nDownloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 5.02kB/s]\nDownloading (…)lve/main/config.json: 100%|██████████| 483/483 [00:00<00:00, 206kB/s]\nDownloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 44.7MB/s]\nDownloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 74.1MB/s]\ncreated the tokenizer\nDownloading (…)\"pytorch_model.bin\";: 100%|██████████| 268M/268M [00:00<00:00, 307MB/s]\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/root/venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\ndefined the model\n","output_type":"stream"},{"output_type":"error","ename":"KernelInterrupted","evalue":"Execution interrupted by the Jupyter kernel.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKernelInterrupted\u001b[0m: Execution interrupted by the Jupyter kernel."]}],"execution_count":1},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=36b8a784-1bd6-47ec-836d-38cd0b38ef5c' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"tags":[],"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"bb390326339947c59c487596553ccff9","deepnote_execution_queue":[]}}